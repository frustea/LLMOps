{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf11017-90e6-42ef-93cb-b6b676f514a3",
   "metadata": {},
   "source": [
    "# Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fcd000-f502-47e5-b0db-809b9555f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "\n",
    "# Ignore FutureWarnings in kfp\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \n",
    "                        category=FutureWarning, \n",
    "                        module='kfp.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcbbe99-c886-4e03-8a72-633b76d8b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e4334e-8c76-49e0-9b6b-309fe3cbc204",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Automation and Orchestration of a Supervised Tuning Pipeline.\n",
    "\n",
    "- use an existing Kubeflow Pipeline for Parameter-Efficient Fine-Tuning (PEFT) for a foundation model from Google, called [PaLM 2](https://ai.google/discover/palm2/). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88178549-83df-4fa8-9554-a42e96558d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### these are the same \n",
    "### jsonl files from the previous lab\n",
    "### time stamps have been removed so that \n",
    "### the files are consistent for all learners\n",
    "TRAINING_DATA_URI = \"./tune_data_stack_overflow_python_qa.jsonl\" \n",
    "EVAUATION_DATA_URI = \"./tune_eval_data_stack_overflow_python_qa.jsonl\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd1f73-9818-4e91-99e2-12bd5b02ca03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### path to the pipeline file to reuse\n",
    "### the file is provided in your workspace as well\n",
    "template_path = 'https://us-kfp.pkg.dev/ml-pipeline/\\\n",
    "large-language-model-pipelines/tune-large-model/v2.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0a586-56df-4628-a6c7-b5b823b7168e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95fea7c-e1d4-4d55-bbce-77895e3a5320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date = datetime.datetime.now().strftime(\"%H:%d:%m:%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34abb24-208c-46ad-b5dc-85a731436cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = f\"deep-learning-ai-model-{date}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a637b-6f50-478d-8a58-8e0fc3b8b8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING_STEPS = 200\n",
    "EVALUATION_INTERVAL = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf62a0-b7b9-4fe0-b56c-2518d7a6e672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import authenticate\n",
    "credentials, PROJECT_ID = authenticate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8044cd-cdd3-4a36-972a-e3f6596368e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8deddf9-abad-403d-9780-0a4b4ecbca41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_arguments = {\n",
    "    \"model_display_name\": MODEL_NAME,\n",
    "    \"location\": REGION,\n",
    "    \"large_model_reference\": \"text-bison@001\",\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"train_steps\": TRAINING_STEPS,\n",
    "    \"dataset_uri\": TRAINING_DATA_URI,\n",
    "    \"evaluation_interval\": EVALUATION_INTERVAL,\n",
    "    \"evaluation_data_uri\": EVAUATION_DATA_URI,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf05e9-40d2-474d-9d43-c9e644bf6053",
   "metadata": {},
   "source": [
    "```Python\n",
    "pipeline_root \"./\"\n",
    "\n",
    "job = PipelineJob(\n",
    "        ### path of the yaml file to execute\n",
    "        template_path=template_path,\n",
    "        ### name of the pipeline\n",
    "        display_name=f\"deep_learning_ai_pipeline-{date}\",\n",
    "        ### pipeline arguments (inputs)\n",
    "        parameter_values=pipeline_arguments,\n",
    "        ### region of execution\n",
    "        location=REGION,\n",
    "        ### root is where temporary files are being \n",
    "        ### stored by the execution engine\n",
    "        pipeline_root=pipeline_root,\n",
    "        ### enable_caching=True will save the outputs \n",
    "        ### of components for re-use, and will only re-run those\n",
    "        ### components for which the code or data has changed.\n",
    "        enable_caching=True,\n",
    ")\n",
    "\n",
    "### submit for execution\n",
    "job.submit()\n",
    "\n",
    "### check to see the status of the job\n",
    "job.state\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7facc65c-f84b-40e8-876e-b2faa2a61135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b61034-7243-4e03-8a46-9ef89a70bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project = PROJECT_ID,\n",
    "              location = REGION,\n",
    "              credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tuned_models = model.list_tuned_model_names()\n",
    "import random\n",
    "tuned_model_select = random.choice(list_tuned_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model = TextGenerationModel.get_tuned_model\\\n",
    "(tuned_model_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"How load file from pickle?\"\n",
    "response = deployed_model.predict(PROMPT)\n",
    "print(response)\n",
    "### retrieve the \"content\" key from the second object\n",
    "final_output = response._prediction_response[0][0][\"content\"]\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION = \"\"\"\\\n",
    "Please answer the following Stackoverflow question on Python.\\\n",
    "Answer it like\\\n",
    "you are a developer answering Stackoverflow questions.\\\n",
    "Question:\n",
    "\"\"\"\n",
    "QUESTION = \"How can I store my data as pickle file? Python example?\"\n",
    "\n",
    "PROMPT = f\"\"\"\n",
    "{INSTRUCTION} {QUESTION}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response = deployed_model.predict(PROMPT)\n",
    "output = final_response._prediction_response[0][0][\"content\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### retrieve the \"blocked\" key from the \n",
    "### \"safetyAttributes\" of the response\n",
    "blocked = response._prediction_response[0][0]\\\n",
    "['safetyAttributes']['blocked']\n",
    "print(blocked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "### retrieve the \"safetyAttributes\" of the response\n",
    "safety_attributes = response._prediction_response[0][0]\\\n",
    "['safetyAttributes']\n",
    "pprint(safety_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### retrieve the \"citations\" key from the \n",
    "### \"citationMetadata\" of the response\n",
    "citation = response._prediction_response[0][0]\\\n",
    "['citationMetadata']['citations']\n",
    "pprint(citation)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m113"
  },
  "kernelspec": {
   "display_name": "python3811",
   "language": "python",
   "name": "python3811"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
